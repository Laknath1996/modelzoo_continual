{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MNIST Task Incremental Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from typing import Any, Tuple\n",
    "\n",
    "from typing import List\n",
    "from copy import deepcopy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Base CNN architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the base CNN\n",
    "class SmallConv(nn.Module):\n",
    "    \"\"\"\n",
    "    Small convolution network with no residual connections\n",
    "    \"\"\"\n",
    "    def __init__(self, num_task=1, num_cls=10, channels=3,\n",
    "                 avg_pool=2, lin_size=320):\n",
    "        super(SmallConv, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(channels, 80, kernel_size=3, bias=False)\n",
    "        self.conv2 = nn.Conv2d(80, 80, kernel_size=3)\n",
    "        self.bn2 = nn.BatchNorm2d(80)\n",
    "        self.conv3 = nn.Conv2d(80, 80, kernel_size=3)\n",
    "        self.bn3 = nn.BatchNorm2d(80)\n",
    "\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.maxpool = nn.MaxPool2d(avg_pool)\n",
    "\n",
    "        self.linsize = lin_size\n",
    "\n",
    "        lin_layers = []\n",
    "        for task in range(num_task):\n",
    "            lin_layers.append(nn.Linear(self.linsize, num_cls)) # add fully connected layers for each task\n",
    "\n",
    "        self.fc = nn.ModuleList(lin_layers) # holds the task specific FC \n",
    "\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                m.bias.data.zero_()\n",
    "\n",
    "    def forward(self, x, tasks):\n",
    "\n",
    "        x = self.conv1(x)\n",
    "        x = self.maxpool(self.relu(x))\n",
    "\n",
    "        x = self.conv2(x)\n",
    "        x = self.maxpool(self.relu(self.bn2(x)))\n",
    "\n",
    "        x = self.conv3(x)\n",
    "        x = self.maxpool(self.relu(self.bn3(x)))\n",
    "        x = x.view(-1, self.linsize)\n",
    "\n",
    "        logits = self.fc[0](x) * 0 # get a zero-vector\n",
    "\n",
    "        for idx, lin in enumerate(self.fc):\n",
    "            task_idx = torch.nonzero((idx == tasks), as_tuple=False).view(-1) # select the training examples in the batch that belongs to the current task\n",
    "            if len(task_idx) == 0: # if there are no training examples for the current task, continue\n",
    "                continue\n",
    "\n",
    "            task_out = torch.index_select(x, dim=0, index=task_idx) # obtain the training examples of the current task\n",
    "            task_logit = lin(task_out) # task-specific FC layer\n",
    "            logits.index_add_(0, task_idx, task_logit) # add the task-specific logits to the full-logits vector\n",
    "\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split MNIST Data-handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModMNIST(torchvision.datasets.MNIST):\n",
    "    def __getitem__(self, index: int) -> Tuple[Any, Any]:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            index (int): Index\n",
    "\n",
    "        Returns:\n",
    "            tuple: (image, target) where target is index of the target class.\n",
    "        \"\"\"\n",
    "        img, target = self.data[index], self.targets[index]\n",
    "\n",
    "        # doing this so that it is consistent with all other datasets\n",
    "        # to return a PIL Image\n",
    "        img = Image.fromarray(img.numpy(), mode='L')\n",
    "\n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)\n",
    "\n",
    "        if self.target_transform is not None:\n",
    "            target = self.target_transform(target)\n",
    "\n",
    "        return img, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SplitMNISTHandler:\n",
    "    \"\"\"\n",
    "    Object for the SplitMNIST dataset\n",
    "    \"\"\"\n",
    "    def __init__(self, tasks):\n",
    "        mean_norm = [0.50]\n",
    "        std_norm = [0.25]\n",
    "        vanilla_transform = transforms.Compose([\n",
    "                            transforms.ToTensor(),\n",
    "                            transforms.Normalize(mean=mean_norm, std=std_norm)])\n",
    "\n",
    "        trainset = ModMNIST('/Users/ashwindesilva/research/modelzoo_continual/notebooks', download=True, train=True, transform=vanilla_transform)\n",
    "        testset = ModMNIST('/Users/ashwindesilva/research/modelzoo_continual/notebooks', download=True, train=False, transform=vanilla_transform)\n",
    "\n",
    "        tr_ind, te_ind = [], []\n",
    "        tr_lab, te_lab = [], []\n",
    "        for task_id, tsk in enumerate(tasks):\n",
    "            for lab_id, lab in enumerate(tsk):\n",
    "\n",
    "                task_tr_ind = np.where(np.isin(trainset.targets,\n",
    "                                                [lab % 10]))[0]\n",
    "                task_te_ind = np.where(np.isin(testset.targets,\n",
    "                                                [lab % 10]))[0]\n",
    "\n",
    "                tr_ind.append(task_tr_ind)\n",
    "                te_ind.append(task_te_ind)\n",
    "                curlab = (task_id, lab_id)\n",
    "\n",
    "                tr_vals = [curlab for _ in range(len(task_tr_ind))]\n",
    "                te_vals = [curlab for _ in range(len(task_te_ind))]\n",
    "\n",
    "                tr_lab.append(tr_vals)\n",
    "                te_lab.append(te_vals)\n",
    "\n",
    "        tr_ind, te_ind = np.concatenate(tr_ind), np.concatenate(te_ind)\n",
    "        tr_lab, te_lab = np.concatenate(tr_lab), np.concatenate(te_lab)\n",
    "\n",
    "        trainset.data = trainset.data[tr_ind]\n",
    "        testset.data = testset.data[te_ind]\n",
    "\n",
    "        trainset.targets = [list(it) for it in tr_lab]\n",
    "        testset.targets = [list(it) for it in te_lab]\n",
    "\n",
    "        self.trainset = trainset\n",
    "        self.testset = testset\n",
    "\n",
    "    def get_data_loader(self, batch_size, train=True):\n",
    "        def wif(id):\n",
    "            \"\"\"\n",
    "            Used to fix randomization bug for pytorch dataloader + numpy\n",
    "            Code from https://github.com/pytorch/pytorch/issues/5059\n",
    "            \"\"\"\n",
    "            process_seed = torch.initial_seed()\n",
    "            # Back out the base_seed so we can use all the bits.\n",
    "            base_seed = process_seed - id\n",
    "            ss = np.random.SeedSequence([id, base_seed])\n",
    "            # More than 128 bits (4 32-bit words) would be overkill.\n",
    "            np.random.seed(ss.generate_state(4))\n",
    "        if train:\n",
    "            data_loader = DataLoader(self.trainset, batch_size=batch_size, shuffle=True, worker_init_fn=wif, pin_memory=True, num_workers=4)\n",
    "        else:\n",
    "            data_loader = DataLoader(self.testset, batch_size=batch_size, shuffle=False, worker_init_fn=wif, pin_memory=True, num_workers=4)\n",
    "        return data_loader\n",
    "\n",
    "    def get_task_data_loader(self, task, batch_size, train=False):\n",
    "        \"\"\"\n",
    "        Get Dataloader for a specific task\n",
    "        \"\"\"\n",
    "        def wif(id):\n",
    "            \"\"\"\n",
    "            Used to fix randomization bug for pytorch dataloader + numpy\n",
    "            Code from https://github.com/pytorch/pytorch/issues/5059\n",
    "            \"\"\"\n",
    "            process_seed = torch.initial_seed()\n",
    "            # Back out the base_seed so we can use all the bits.\n",
    "            base_seed = process_seed - id\n",
    "            ss = np.random.SeedSequence([id, base_seed])\n",
    "            # More than 128 bits (4 32-bit words) would be overkill.\n",
    "            np.random.seed(ss.generate_state(4))\n",
    "        if train:\n",
    "            task_set = deepcopy(self.trainset)\n",
    "        else:\n",
    "            task_set = deepcopy(self.testset)\n",
    "\n",
    "        task_ind = [task == i[0] for i in task_set.targets]\n",
    "\n",
    "        task_set.data = task_set.data[task_ind]\n",
    "        task_set.targets = np.array(task_set.targets)[task_ind, :]\n",
    "        task_set.targets = [(lab[0], lab[1]) for lab in task_set.targets]\n",
    "\n",
    "        loader = DataLoader(\n",
    "            task_set, batch_size=batch_size,\n",
    "            shuffle=False, num_workers=6, pin_memory=True,\n",
    "            worker_init_fn=wif)\n",
    "\n",
    "        return loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-Head Learner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHead():\n",
    "    \"\"\"\n",
    "    Object for initializing and training a multihead learner\n",
    "    \"\"\"\n",
    "    def __init__(self, args, hp, data_conf):\n",
    "        \"\"\"\n",
    "        Initialize multihead learner\n",
    "\n",
    "        Params:\n",
    "          - args:      Arguments from arg parse\n",
    "          - hp:        dict of hyper-parameters config\n",
    "          - data_conf: dcit of dataset config\n",
    "        \"\"\"\n",
    "        self.args = args\n",
    "        self.hp = hp\n",
    "\n",
    "        num_tasks = len(data_conf['tasks'])\n",
    "        num_classes = len(data_conf['tasks'][0])\n",
    "\n",
    "        # Random seed\n",
    "        torch.manual_seed(abs(args['seed']))\n",
    "        np.random.seed(abs(args['seed']))\n",
    "\n",
    "        # Initialize Network. code assumes all tasks have same no. of classes\n",
    "        self.net = SmallConv(\n",
    "                    num_task=num_tasks, \n",
    "                    num_cls=num_classes,\n",
    "                    channels=1, \n",
    "                    avg_pool=2,\n",
    "                    lin_size=80)\n",
    "\n",
    "        # Get dataset\n",
    "        dataset = SplitMNISTHandler(data_conf['tasks'])\n",
    "        self.train_loader = dataset.get_data_loader(hp['batch_size'], train=True)\n",
    "\n",
    "        # Loss and Optimizer\n",
    "        self.optimizer = torch.optim.SGD(self.net.parameters(), lr=hp['lr'],\n",
    "                                   momentum=0.9, nesterov=True,\n",
    "                                   weight_decay=hp['l2_reg'])\n",
    "        self.lr_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "            self.optimizer, args['epochs'] * len(self.train_loader))\n",
    "\n",
    "    def train(self):\n",
    "        \"\"\"\n",
    "        Train the multi-task learner\n",
    "        \"\"\"\n",
    "        device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "        self.net.to(device)\n",
    "\n",
    "        # Train multi-head model\n",
    "        for epoch in range(self.args['epochs']):\n",
    "            train_loss = 0.0\n",
    "            train_acc = 0.0\n",
    "            batches = 0.0\n",
    "            criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "            self.net.train()\n",
    "\n",
    "            for dat, target in self.train_loader:\n",
    "                self.optimizer.zero_grad()\n",
    "\n",
    "                tasks, labels = target\n",
    "                labels = labels.long()\n",
    "                tasks = tasks.long()\n",
    "                batch_size = int(labels.size()[0])\n",
    "\n",
    "                dat = dat.to(device)\n",
    "                labels = labels.to(device)\n",
    "                tasks = tasks.to(device)\n",
    "\n",
    "                # Forward/Back-prop\n",
    "                out = self.net(dat, tasks)\n",
    "                loss = criterion(out, labels)\n",
    "                loss.backward()\n",
    "\n",
    "                self.optimizer.step()\n",
    "\n",
    "                self.lr_scheduler.step()\n",
    "\n",
    "                # Compute Train metrics\n",
    "                batches += batch_size\n",
    "                train_loss += loss.item() * batch_size\n",
    "                labels = labels.cpu().numpy()\n",
    "                out = out.cpu().detach().numpy()\n",
    "                train_acc += np.sum(labels == (np.argmax(out, axis=1)))\n",
    "\n",
    "            print(\"Epoch = {}\".format(epoch))\n",
    "            print(\"Train loss = {:3f}\".format(train_loss/batches))\n",
    "            print(\"Train acc = {:3f}\".format(train_acc/batches))\n",
    "            print(\"\\n\")\n",
    "            \n",
    "        return self.net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Zoo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelZoo():\n",
    "    \"\"\"\n",
    "    Object for the modelzoo\n",
    "    \"\"\"\n",
    "    def __init__(self, args, data_conf, hp_conf):\n",
    "        self.args = args\n",
    "        self.tasks_info = data_conf['tasks']\n",
    "        self.num_tasks = len(self.tasks_info)\n",
    "        \n",
    "        self.data_conf = data_conf\n",
    "        self.hp_conf = hp_conf\n",
    "\n",
    "        self.wts = np.array([1.0 for i in range(self.num_tasks)])\n",
    "        self.learner_task_idx = []\n",
    "\n",
    "        # Random generator for sampling tasks in every boosting iteration\n",
    "        self.rng = np.random.default_rng(seed=100)\n",
    "\n",
    "        # Store train and test predictions of individual models\n",
    "        self.tr_preds = {}\n",
    "        self.te_preds = {}\n",
    "        for t_id in range(self.num_tasks):\n",
    "            self.te_preds[t_id] = []\n",
    "            self.tr_preds[t_id] = []\n",
    "\n",
    "        # Store the individual models at each round\n",
    "        self.modelzoo = {}\n",
    "        self.synergistic_tasks = {}\n",
    "\n",
    "    def sample_tasks(self, rounds):\n",
    "        numsubtasks = min(2, rounds + 0) # number of tasks sampled at each round\n",
    "        pr = self.wts[:rounds] / np.sum(self.wts[:rounds])\n",
    "        if rounds != 0:\n",
    "            learner_task_idx = self.rng.choice(rounds,\n",
    "                                               numsubtasks - 1,\n",
    "                                               replace=False, p=pr) # sampling the tasks using a multinomial distribution\n",
    "        else:\n",
    "            learner_task_idx = np.array([])\n",
    "\n",
    "        # Manually add the newly seen task (boosting should\n",
    "        # automatically select this task due to the the very large loss)\n",
    "        learner_task_idx = np.append(learner_task_idx, int(rounds))\n",
    "        learner_task_idx = np.array(learner_task_idx, dtype=np.int32)\n",
    "\n",
    "        learner_task_info = [] # store the info of the samples tasks\n",
    "        for idx in learner_task_idx:\n",
    "            learner_task_info.append(self.tasks_info[idx])\n",
    "\n",
    "        self.learner_task_info = learner_task_info\n",
    "        self.learner_task_idx = learner_task_idx\n",
    "\n",
    "        # change here\n",
    "        learner_conf = deepcopy(self.data_conf)\n",
    "        learner_conf['tasks'] = deepcopy(learner_task_info)\n",
    "        self.synergistic_tasks[rounds] = learner_task_idx\n",
    "\n",
    "        print(\"\\n====== Round %d ======\" % (rounds + 1))\n",
    "        print(\"Sampled tasks: %s\" % (str(learner_task_idx)))\n",
    "        return learner_conf\n",
    "\n",
    "    def add_learner(self, learner_conf, rounds):\n",
    "        \"\"\"\n",
    "        Add a learner to the model-Zoo\n",
    "\n",
    "        params:\n",
    "          - learner_conf: dict describing Subset of tasks to train with\n",
    "        \"\"\"\n",
    "        # Train a single \"multi-head\" learner and add it to the Model Zoo\n",
    "        model = MultiHead(self.args, self.hp_conf, learner_conf)\n",
    "        net = model.train()\n",
    "        self.modelzoo[rounds] = net\n",
    "\n",
    "        # Store all predictions of learner on train/test dataset \n",
    "        # This allows us to discard the learners weights\n",
    "        tr_ret = self.fetch_outputs(net, learner_conf['tasks'], True)\n",
    "        te_ret = self.fetch_outputs(net, learner_conf['tasks'], False)\n",
    "        for idx, t_id in enumerate(self.learner_task_idx):\n",
    "            self.tr_preds[t_id].append(tr_ret[idx])\n",
    "            self.te_preds[t_id].append(te_ret[idx])\n",
    "\n",
    "    def fetch_outputs(self, net, tasks, tr_flag=False):\n",
    "        \"\"\"\n",
    "        Compute the outputs of newly trained learner on the tasks it\n",
    "        was trained on. The predictions of different learners are not\n",
    "        combined so that they can be used to compute the error of the\n",
    "        Model Zoo at any stage. This allows us to discard the weights\n",
    "        of the individual learner.\n",
    "\n",
    "        params:\n",
    "          - net:         Neural net of newest learner\n",
    "          - l_task_info: Description of subset of tasks that neural net was\n",
    "                         trained on\n",
    "          - tr_flag:     Determines whether to use train/test set\n",
    "        \"\"\"\n",
    "        dataset = SplitMNISTHandler(tasks)\n",
    "        device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "        test_loaders = []\n",
    "        for t_id in range(len(tasks)):\n",
    "            test_loaders.append(\n",
    "                dataset.get_task_data_loader(t_id, 100, train=tr_flag))\n",
    "\n",
    "        task_outputs = []\n",
    "        net.eval()\n",
    "\n",
    "        with torch.inference_mode():\n",
    "            for dataloader in test_loaders:\n",
    "                outputs = []\n",
    "                for dat, target in dataloader:\n",
    "                    tasks, labels = target\n",
    "                    tasks = tasks.long()\n",
    "                    labels = labels.long()\n",
    "\n",
    "                    dat = dat.to(device)\n",
    "                    labels = labels.to(device)\n",
    "                    tasks = tasks.to(device)\n",
    "\n",
    "                    out = net(dat, tasks)\n",
    "                    out = nn.functional.softmax(out, dim=1)\n",
    "                    out = out.cpu().detach().numpy()\n",
    "                    outputs.append(out)\n",
    "                outputs = np.concatenate(outputs)\n",
    "                task_outputs.append(outputs)\n",
    "        return task_outputs\n",
    "\n",
    "    def evaluate(self, rounds: int):\n",
    "        \"\"\"\n",
    "        Evaluate the entire Model Zoo (combination of all learners)\n",
    "        on the train and test sets and log the results\n",
    "\n",
    "        params:\n",
    "          - rounds: Number of learners added to Zoo\n",
    "        \"\"\"\n",
    "        tr_ret = self.evaluate_preds(self.tr_preds, True)\n",
    "        te_ret = self.evaluate_preds(self.te_preds, False)\n",
    "\n",
    "        def rnd(x):\n",
    "            return list(np.round(x, 3))\n",
    "\n",
    "        info = {\n",
    "            'round': rounds,\n",
    "            'TrainLoss': rnd(tr_ret['Loss']),\n",
    "            'TrainAcc': rnd(tr_ret['Accuracy']),\n",
    "            'TestLoss': rnd(te_ret['Loss']),\n",
    "            'TestAcc': rnd(te_ret['Accuracy']),\n",
    "            'last_learner_tasks': list(self.learner_task_idx),\n",
    "            'last_learner_weights': rnd(self.wts)\n",
    "        }\n",
    "\n",
    "        avg_acc = np.mean(info['TrainAcc'][:rounds]) if rounds > 0 else 0.0\n",
    "        allacc = str(list(np.round(info['TrainAcc'][:rounds], 2)))\n",
    "        print(\"Average accuracy of all seen tasks: %.2f\" % (avg_acc))\n",
    "        print(\"Individual accuracies of all seen tasks:\\n%s\" % (allacc))\n",
    "        return tr_ret['Loss'], info\n",
    "\n",
    "    def evaluate_preds(self, preds, tr_flag):\n",
    "        \"\"\"\n",
    "        Use the set of predictions from all learners to compute the error\n",
    "        and the loss of the entire Model Zoo\n",
    "        \"\"\"\n",
    "        dataset = SplitMNISTHandler(self.tasks_info)\n",
    "        criterion = nn.NLLLoss()\n",
    "        numcls = len(self.tasks_info[0])\n",
    "\n",
    "        test_loaders = []\n",
    "        for t_id in range(self.num_tasks):\n",
    "            test_loaders.append(\n",
    "                dataset.get_task_data_loader(t_id, 100, train=tr_flag))\n",
    "\n",
    "        all_loss = []\n",
    "        all_acc = []\n",
    "\n",
    "        # Iterate over tasks and compute error/loss of Model Zoo on each task\n",
    "        for task_id, dataloader in enumerate(test_loaders):\n",
    "            count = 0\n",
    "            acc = 0\n",
    "            loss = 0\n",
    "\n",
    "            # Compute the outputs of the entire Model Zoo by ensemble\n",
    "            # averaging of the predictions of all learners\n",
    "            if len(preds[task_id]) == 0:\n",
    "                # If model has no prediction, output uniform probabilities\n",
    "                numpts = len(dataloader.dataset)\n",
    "                curpred = np.ones((numpts, numcls)) / numcls\n",
    "            else:\n",
    "                # If limited replay was, used apply a weighted ensemble. The\n",
    "                # rationale is that we increase the weight of a learner if it\n",
    "                # trained on more samples. This is true for the first learner\n",
    "                # trained on a task (wts[0] is hence has higher weight)\n",
    "                wts = np.ones(len(preds[task_id]))\n",
    "                wts[0] = 1 / self.args['replay_frac']\n",
    "                curpred = np.average(preds[task_id], axis=0, weights=wts)\n",
    "\n",
    "            # Compute error/loss using outputs of Model Zoo (curpred)\n",
    "            for dat, target in dataloader:\n",
    "                tasks, labels = target\n",
    "                tasks, labels = tasks.long(), labels.long()\n",
    "                batch_size = int(labels.size()[0])\n",
    "\n",
    "                dat = dat.cuda(non_blocking=True)\n",
    "                tasks = tasks.cuda(non_blocking=True)\n",
    "\n",
    "                out = curpred[count:count + batch_size]\n",
    "                out = torch.log(torch.Tensor(out))\n",
    "\n",
    "                loss += (criterion(out, labels).item()) * batch_size\n",
    "\n",
    "                labels = labels.cpu().numpy()\n",
    "                out = out.cpu().detach().numpy()\n",
    "                acc += np.sum(labels == (np.argmax(out, axis=1)))\n",
    "                count += batch_size\n",
    "\n",
    "            all_loss.append(loss / count)\n",
    "            all_acc.append(acc / count)\n",
    "\n",
    "        info = {'Loss': all_loss,\n",
    "                'Accuracy': all_acc,\n",
    "                'train': tr_flag}\n",
    "        return info\n",
    "\n",
    "    def update_task_wts(self, losses):\n",
    "        \"\"\"\n",
    "        Update the sampling weights based on the losses. self.wts should\n",
    "        ideally be based on the transfer exponent $\\rho$. We however, use\n",
    "        the (noramlized) training loss like in boosting\n",
    "\n",
    "        params:\n",
    "          - losses: List of training losses on various tasks\n",
    "        \"\"\"\n",
    "        losses = (losses - np.mean(losses)) / np.mean(losses)\n",
    "        losses = np.exp(losses)\n",
    "        losses = np.clip(losses, 0.0001, 1000)\n",
    "\n",
    "        self.wts = losses\n",
    "        return losses\n",
    "\n",
    "    def train(self):\n",
    "        \"\"\"\n",
    "        Train the Model Zoo\n",
    "        \"\"\"\n",
    "        self.evaluate(0)\n",
    "        for rounds in range(self.num_tasks):\n",
    "            learner_conf = self.sample_tasks(rounds)\n",
    "            self.add_learner(learner_conf, rounds)\n",
    "            losses = self.evaluate(rounds + 0)\n",
    "            self.update_task_wts(losses)\n",
    "\n",
    "    # def predict(self, x, task_ids):\n",
    "    #     \"\"\"\n",
    "    #     Predict using Model Zoo\n",
    "    #     \"\"\"\n",
    "    #     for task in task_ids:\n",
    "    #       for round in self.synergistic_tasks.keys():\n",
    "    #         if task is in self.synergistic_tasks[round]:\n",
    "    #           net = self.modelzoo[round]\n",
    "              "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Model Zoo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "\n",
    "args = {\n",
    "    'seed': 100,\n",
    "    'tasks_per_round': 5,\n",
    "    'epochs': 1,\n",
    "    'replay_frac': 1.0\n",
    "}\n",
    "data_conf = {\n",
    "    'data': 'mnist',\n",
    "    'tasks': [[0, 1], [2, 3], [4, 5], [6, 7], [8, 9]]\n",
    "}\n",
    "\n",
    "hp_conf = {\n",
    "    'batch_size': 16,\n",
    "    'lr': 0.01,\n",
    "    'l2_reg': 1e-5\n",
    "}\n",
    "\n",
    "zoo = ModelZoo(args, data_conf, hp_conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for rounds in range(zoo.num_tasks):\n",
    "    learner_conf = zoo.sample_tasks(rounds)\n",
    "    print(learner_conf)\n",
    "    zoo.add_learner(learner_conf, rounds)\n",
    "    losses, info = zoo.evaluate(rounds + 0)\n",
    "    print(info)\n",
    "    print(len(zoo.te_preds[0]))\n",
    "    zoo.update_task_wts(losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inferece using Model Zoo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "task_ids = [0, 1, 2, 3, 4]\n",
    "tasks = [[0, 1], [2, 3], [4, 5], [6, 7], [8, 9]]\n",
    "dataset = SplitMNISTHandler(tasks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loaders = []\n",
    "for t_id in task_ids:\n",
    "    test_loaders.append(dataset.get_task_data_loader(t_id, 100, train=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_network_output(net, dataloader):\n",
    "    net.eval()\n",
    "    device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "    net.to(device)\n",
    "    outputs = []\n",
    "    with torch.inference_mode():\n",
    "        for dat, target in dataloader:\n",
    "            tasks, labels = target\n",
    "            tasks = tasks.long()\n",
    "            labels = labels.long()\n",
    "\n",
    "            dat = dat.to(device)\n",
    "            labels = labels.to(device)\n",
    "            tasks = tasks.to(device)\n",
    "\n",
    "            out = net(dat, tasks)\n",
    "            out = nn.functional.softmax(out, dim=1)\n",
    "            out = out.cpu().detach().numpy()\n",
    "            outputs.append(out)\n",
    "    outputs = np.concatenate(outputs)\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = {}\n",
    "for t_id in range(len(tasks)):\n",
    "    preds[t_id] = []\n",
    "for t_id in task_ids:\n",
    "    print(\"Task ID : {}\".format(t_id))\n",
    "    dataloader = test_loaders[t_id]\n",
    "    for round in zoo.synergistic_tasks.keys():\n",
    "        if t_id in zoo.synergistic_tasks[round]:\n",
    "            print(round)\n",
    "            net = zoo.modelzoo[round]\n",
    "            outputs = get_network_output(net, dataloader)\n",
    "            preds[t_id].append(outputs)\n",
    "        else:\n",
    "          continue"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b7ce79920bed3c50702efbb6483628d0b7e9ab2a932efff77f3cc7e0a21d7b28"
  },
  "kernelspec": {
   "display_name": "Python 3.9.9 64-bit ('kdg_env': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
